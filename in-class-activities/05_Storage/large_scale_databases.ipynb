{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Large-Scale Database Solutions\n",
    "## DynamoDB\n",
    "First, let's create a DynamoDB table. Let's say that we're collecting and storing streaming Twitter (now X) data in our database. We'll use Twitter 'username' as our primary key here, since this will be unique to each user and will make for a good input for DynamoDB's hash function (you can also specify a sort key if you would like, though). We'll also set our Read and Write Capacity down to the minimum for this demo, but you can [scale this up](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html) if you need more throughput for your application (just be careful, as increasing your Read/Write Capacity too far will rapidly deplete your AWS credits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2024-04-10 12:55:15.656000-05:00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "table = dynamodb.create_table(\n",
    "    TableName='twitter',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'username',\n",
    "            'KeyType': 'HASH'\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'username',\n",
    "            'AttributeType': 'S'\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 1,\n",
    "        'WriteCapacityUnits': 1\n",
    "    }    \n",
    ")\n",
    "\n",
    "# Wait until AWS confirms that table exists before moving on\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='twitter')\n",
    "\n",
    "# get data about table (should currently be no items in table)\n",
    "print(table.item_count)\n",
    "print(table.creation_date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we currently have an empty DynamoDB table. Let's actually put some items into our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '98Q3E8R8750LK6KGVUKIANNAN3VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 10 Apr 2024 17:57:24 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '98Q3E8R8750LK6KGVUKIANNAN3VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2745614147'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.put_item(\n",
    "   Item={\n",
    "        'username': 'macs30123',\n",
    "        'num_followers': 100,\n",
    "        'num_tweets': 5\n",
    "    }\n",
    ")\n",
    "\n",
    "table.put_item(\n",
    "   Item={\n",
    "        'username': 'jon_c',\n",
    "        'num_followers': 10,\n",
    "        'num_tweets': 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily get items from our table using the `get_item` method and providing our key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_followers': Decimal('100'), 'username': 'macs30123', 'num_tweets': Decimal('5')}\n"
     ]
    }
   ],
   "source": [
    "response = table.get_item(\n",
    "    Key={\n",
    "        'username': 'macs30123'\n",
    "    }\n",
    ")\n",
    "item = response['Item']\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also update existing items using the `update_item` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'C2GEU1UABJ3J7O85SDR0C7OT0FVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 10 Apr 2024 17:57:29 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'C2GEU1UABJ3J7O85SDR0C7OT0FVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2745614147'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.update_item(\n",
    "    Key={\n",
    "        'username': 'macs30123'\n",
    "    },\n",
    "    UpdateExpression='SET num_tweets = :val1',\n",
    "    ExpressionAttributeValues={\n",
    "        ':val1': 6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we take a look again at this item, we'll see that it's been updated (note, though, that DynamoDB tables are [*eventually consistent* unless we specify otherwise](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html), so this might not always return the expected result immediately):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_followers': Decimal('100'), 'username': 'macs30123', 'num_tweets': Decimal('6')}\n"
     ]
    }
   ],
   "source": [
    "response = table.get_item(\n",
    "    Key={\n",
    "        'username': 'macs30123'\n",
    "    }\n",
    ")\n",
    "item = response['Item']\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as well, that even though it is not optimal to perform complicated queries in DynamoDB tables, we can write and run SQL-like queries to run again our DynamoDB tables if we want to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'num_followers': Decimal('100'), 'username': 'macs30123', 'num_tweets': Decimal('6')}]\n"
     ]
    }
   ],
   "source": [
    "response = table.meta.client.execute_statement(\n",
    "    Statement='''\n",
    "              SELECT *\n",
    "              FROM twitter\n",
    "              WHERE num_followers > 20\n",
    "              '''\n",
    ")\n",
    "item = response['Items']\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed we wanted to gather data, perform pre-processing steps, and then enter into our database -- all in the cloud. To do this, we can use `boto3` to access our DynamoDB database from within other AWS resources (such as Lambda or EC2). For instance, let's create a Lambda function that will process some data (username, as well raw follower and tweet data) and enter the results of this processing into our database without ever leaving the AWS cloud (see zipped Lambda deployment package in this directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Lambda client\n",
    "aws_lambda = boto3.client('lambda')\n",
    "\n",
    "# Access our class IAM role, which allows Lambda\n",
    "# to interact with other AWS resources\n",
    "iam_client = boto3.client('iam')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "\n",
    "# Open our Zipped directory\n",
    "with open('write_to_dynamodb.zip', 'rb') as f:\n",
    "    lambda_zip = f.read()\n",
    "\n",
    "try:\n",
    "    # If function hasn't yet been created, create it\n",
    "    response = aws_lambda.create_function(\n",
    "        FunctionName='write_to_dynamodb',\n",
    "        Runtime='python3.9',\n",
    "        Role=role['Role']['Arn'],\n",
    "        Handler='lambda_function.lambda_handler',\n",
    "        Code=dict(ZipFile=lambda_zip),\n",
    "        Timeout=3\n",
    "    )\n",
    "except aws_lambda.exceptions.ResourceConflictException:\n",
    "    # If function already exists, update it based on zip\n",
    "    # file contents\n",
    "    response = aws_lambda.update_function_code(\n",
    "    FunctionName='write_to_dynamodb',\n",
    "    ZipFile=lambda_zip\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Go into console to demonstrate test case and check DynamoDB table for new value)\n",
    "\n",
    "Finally, you should make sure to delete your table (if you no longer plan to use it), so that you do not incur further charges while it is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TableDescription': {'TableName': 'twitter',\n",
       "  'TableStatus': 'DELETING',\n",
       "  'ProvisionedThroughput': {'NumberOfDecreasesToday': 0,\n",
       "   'ReadCapacityUnits': 1,\n",
       "   'WriteCapacityUnits': 1},\n",
       "  'TableSizeBytes': 0,\n",
       "  'ItemCount': 0,\n",
       "  'TableArn': 'arn:aws:dynamodb:us-east-1:557316176944:table/twitter',\n",
       "  'TableId': '5ef3c655-af13-498d-b136-1022d5d7a50e',\n",
       "  'DeletionProtectionEnabled': False},\n",
       " 'ResponseMetadata': {'RequestId': 'CKAU40P1JVJ8QCNGBRNLURJ2B7VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 10 Apr 2024 18:01:31 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '350',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'CKAU40P1JVJ8QCNGBRNLURJ2B7VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '1561284898'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS\n",
    "\n",
    "We can also create and interact with scalable cloud relational databases via `boto3`. Let's launch a MySQL database via AWS's RDS service. Note that we can explicitly scale up the hardware (e.g. instance class, and allocated storage) for our database via the `create_db_instance` parameters. We can also add additional read replicas of our database instance that we launch via [the `create_db_instance_read_replica` method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html#RDS.Client.create_db_instance_read_replica) or create a cluster of a certain size from the start using [the `create_db_cluster` method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html#RDS.Client.create_db_cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relational-db is available at relational-db.cz0auzktt3ts.us-east-1.rds.amazonaws.com on Port 3306\n"
     ]
    }
   ],
   "source": [
    "rds = boto3.client('rds')\n",
    "\n",
    "response = rds.create_db_instance(\n",
    "    DBInstanceIdentifier='relational-db',\n",
    "    DBName='twitter',\n",
    "    MasterUsername='username',\n",
    "    MasterUserPassword='password',\n",
    "    DBInstanceClass='db.t3.micro',\n",
    "    Engine='MySQL',\n",
    "    AllocatedStorage=5\n",
    ")\n",
    "\n",
    "# Wait until DB is available to continue\n",
    "rds.get_waiter('db_instance_available').wait(DBInstanceIdentifier='relational-db')\n",
    "\n",
    "# Describe where DB is available and on what port\n",
    "db = rds.describe_db_instances()['DBInstances'][0]\n",
    "ENDPOINT = db['Endpoint']['Address']\n",
    "PORT = db['Endpoint']['Port']\n",
    "DBID = db['DBInstanceIdentifier']\n",
    "\n",
    "print(DBID,\n",
    "      \"is available at\", ENDPOINT,\n",
    "      \"on Port\", PORT,\n",
    "     )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access our MySQL database, we'll need to adjust some security settings associated with our server, though. By default, we're not able to access port 3306 on our database server over the internet and we will need to change this setting in order to connect to our database from our local machine. In practice, you should limit the allowed IP range as much as possible (to your home or office, for example) to avoid intruders from connecting to your databases. For the purposes of this demo, though, I am going to make it possible to connect to my database from anywhere on the internet (IP range 0.0.0.0/0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Name of Security Group\n",
    "SGNAME = db['VpcSecurityGroups'][0]['VpcSecurityGroupId']\n",
    "\n",
    "# Adjust Permissions for that security group so that we can access it on Port 3306\n",
    "# If already SG is already adjusted, print this out\n",
    "try:\n",
    "    ec2 = boto3.client('ec2')\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "            GroupId=SGNAME,\n",
    "            IpPermissions=[\n",
    "                {'IpProtocol': 'tcp',\n",
    "                 'FromPort': PORT,\n",
    "                 'ToPort': PORT,\n",
    "                 'IpRanges': [{'CidrIp': '0.0.0.0/0'}]}\n",
    "            ]\n",
    "    )\n",
    "except ec2.exceptions.ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == 'InvalidPermission.Duplicate':\n",
    "        print(\"Permissions already adjusted.\")\n",
    "    else:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we're ready to connect to our database! This is a MySQL database, so let's install a Python package that will allow us to effectively handle this connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mysql-connector-python # Install mysql-connector if you haven't already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can just connect to the database and run queries in the same way that you have seen while working with SQLite databases (using the SQLite3 package). Very cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "conn =  mysql.connector.connect(host=ENDPOINT, user=\"username\", passwd=\"password\", port=PORT, database='twitter')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = '''\n",
    "               CREATE TABLE IF NOT EXISTS users (\n",
    "                   username VARCHAR(10),\n",
    "                   num_followers INT,\n",
    "                   num_tweets INT,\n",
    "                   PRIMARY KEY (username)\n",
    "               )\n",
    "               '''\n",
    "insert_data  = '''\n",
    "               INSERT INTO users (username, num_followers, num_tweets)\n",
    "               VALUES \n",
    "                   ('macs30123', 100, 5),\n",
    "                   ('jon_c', 10, 0)\n",
    "               '''\n",
    "\n",
    "for op in [create_table, insert_data]:\n",
    "    cur.execute(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our relational database is optimized for performing small, fast queries like these and will tend to out-perform our DynamoDB table at these kinds of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jon_c', 10, 0), ('macs30123', 100, 5)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT * FROM users''')\n",
    "query_results = cur.fetchall()\n",
    "print(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('macs30123',)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT username FROM users WHERE num_followers > 20''')\n",
    "query_results = cur.fetchall()\n",
    "print(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done executing SQL queries on our MySQL database, we can close our connection to the database and delete the database on AWS so that we're no longer charged for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting\n",
      "RDS Database has been deleted\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "response = rds.delete_db_instance(DBInstanceIdentifier='relational-db',\n",
    "                       SkipFinalSnapshot=True\n",
    "                      )\n",
    "print(response['DBInstance']['DBInstanceStatus'])\n",
    "\n",
    "# wait until DB is deleted before proceeding\n",
    "rds.get_waiter('db_instance_deleted').wait(DBInstanceIdentifier='relational-db')\n",
    "print(\"RDS Database has been deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Warehousing with Redshift\n",
    "\n",
    "[**Note: Redshift is not support in this year's AWS Academy accounts.** The following code can be run in a personal AWS account.]\n",
    "\n",
    "When you need to run especially big queries against large datasets, it can make sense to perform these in a Data Warehouse like AWS Redshift. Recall that Redshift clusters organize our data in columnar storage (instead of rows, like a standard relational database) and can efficiently perform operations on these columns in parallel.\n",
    "\n",
    "Let's spin up a Redshift cluster to see how this works (for our small Twitter demonstration data). Notice that we do need to provide the particular type of hardware that we want each one of our nodes to be, as well as the number of nodes that we want to include in our cluster (we can increase this for greater parallelism and storage capacity). For this demo, let's just select a two of one of the smaller nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mycluster is available at mycluster.cplerylwuinc.us-east-1.redshift.amazonaws.com on Port 5439\n"
     ]
    }
   ],
   "source": [
    "redshift = boto3.client('redshift')\n",
    "\n",
    "response = redshift.create_cluster(\n",
    "    ClusterIdentifier='myCluster',\n",
    "    DBName='twitter',\n",
    "    NodeType='dc2.large',\n",
    "    NumberOfNodes=2,\n",
    "    MasterUsername='username',\n",
    "    MasterUserPassword='Password123'\n",
    ")\n",
    "\n",
    "# Wait until cluster is available before proceeding\n",
    "redshift.get_waiter('cluster_available').wait(ClusterIdentifier='myCluster')\n",
    "\n",
    "# Describe where cluster is available and on what port\n",
    "cluster = redshift.describe_clusters(ClusterIdentifier='myCluster')['Clusters'][0]\n",
    "ENDPOINT = cluster['Endpoint']['Address']\n",
    "PORT = cluster['Endpoint']['Port']\n",
    "CLUSTERID = cluster['ClusterIdentifier']\n",
    "\n",
    "print(CLUSTERID,\n",
    "      \"is available at\", ENDPOINT,\n",
    "      \"on Port\", PORT,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll need to make sure that we can connect with our cluster from our local machine. For the purposes of this demo, we'll open the port up to the Internet (although, again, you should only allow a narrow IP range in your own applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Name of Security Group\n",
    "SGNAME = cluster['VpcSecurityGroups'][0]['VpcSecurityGroupId']\n",
    "\n",
    "# Adjust Permissions for that security group so that we can access it on Port 5439\n",
    "# If already SG is already adjusted, print this out\n",
    "try:\n",
    "    ec2 = boto3.client('ec2')\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "            GroupId=SGNAME,\n",
    "            IpPermissions=[\n",
    "                {'IpProtocol': 'tcp',\n",
    "                 'FromPort': PORT,\n",
    "                 'ToPort': PORT,\n",
    "                 'IpRanges': [{'CidrIp': '0.0.0.0/0'}]}\n",
    "            ]\n",
    "    )\n",
    "except ec2.exceptions.ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == 'InvalidPermission.Duplicate':\n",
    "        print(\"Permissions already adjusted.\")\n",
    "    else:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redshift was originally forked from PostgreSQL, so the best way to connect with it is via a PostgreSQL Python adaptor (rather than the MySQL adaptor we used previously). We'll use `psycopg2` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that once we import the package and connect, we can use the same workflow that we used for our MySQL database (and our local SQLite databases) to execute SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(dbname='twitter', host=ENDPOINT, user=\"username\", password=\"Password123\", port=PORT)\n",
    "cur = conn.cursor()\n",
    "\n",
    "for op in [create_table, insert_data]:\n",
    "    cur.execute(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('macs30123', 100, 5), ('jon_c', 10, 0)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT * FROM users''')\n",
    "query_results = cur.fetchall()\n",
    "print(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('macs30123',)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT username FROM users WHERE num_followers > 20''')\n",
    "query_results = cur.fetchall()\n",
    "print(query_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once we're done, we can close our connection and delete our Redshift cluster in the same way as our RDS instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting\n",
      "Redshift Cluster has been deleted\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "response = redshift.delete_cluster(ClusterIdentifier='myCluster',\n",
    "                       SkipFinalClusterSnapshot=True\n",
    "                      )\n",
    "print(response['Cluster']['ClusterStatus'])\n",
    "\n",
    "redshift.get_waiter('cluster_deleted').wait(ClusterIdentifier='myCluster')\n",
    "print(\"Redshift Cluster has been deleted\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
